---
title: <center> <h1> Homework 5 </h1> </center>
mainfont: Arial
author: "Boxiang Jia"
date: December 4, 2018
output: html_document
runtime: shiny 
fontsize: 12pt
geometry: margin=1in
self_contained: yes
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{homework 5 vignette}
-->

#Question 1
```{r}
# install and load packages
library(keras)
# install_keras() if doesn't have the keras and tensorflow package
library(keras)
library(glmnet)
library(moments)
library(dplyr)
library(ggplot2)
library(tibble)

# split the data into discovey and validation set, and reshape 
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

x_train <- array_reshape(x_train, c(60000, 28^2))
x_test <- array_reshape(x_test, c(10000, 28^2))
y_train <- factor(y_train)
y_test <- factor(y_test)

# build the model
set.seed(1234)

s <- sample(seq_along(y_train), 1000)
fit <- cv.glmnet(x_train[s,], y_train[s], family = "multinomial")

# fit the model into the validation set
preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda.min,  type = "class")
t <- table(as.vector(preds), y_test)
sum(diag(t)) / sum(t)
```
The prediction accuracy of the way above is 0.8556, and to increase the out of sample predication accuracy,
we may add other features, credit to the thoughts from Guoyin and online search, we will add mean and standard deviation
as the new featurea to predict the model.

```{r}
x_train2 <- cbind(x_train, as.vector(apply(x_train, 1, mean)), as.vector(apply(x_train, 1, sd)))
x_test2 <- cbind(x_test, as.vector(apply(x_test, 1, mean)), as.vector(apply(x_test, 1, sd)))

# build the  model and predict
fit2 <- cv.glmnet(x_train2[s,], y_train[s], family = "multinomial")
preds2 <- predict(fit2$glmnet.fit, x_test2, s = fit2$lambda.min, type = "class")
t2 <- table(as.vector(preds2), y_test)
sum(diag(t2)) / sum(t2)
```
The accuracy has been improved from 0.8556 to 0.8566, which is a slight improvement.
Moreover, we could also use the Convolutional Neural Network(CNN) to predict. I actually do not have an idea how to perform it.
Credit to the code of BIS557 classmate Hongyu Li: https://github.com/hongyuli94/bis557/blob/master/vignettes/homework-5.rmd
I forked the code here, and add some revision to make it  more conise and run more quickly.

```{r}
# set the dimensions
img_rows <- 28
img_cols <- 28
num_classes <- 10
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)

# refine RGB value, setting between 0 and 1
x_train <- x_train / 255
x_test <- x_test / 255

# Convert class vectors to binary 
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)

# build the model of sequantial analysis
model_0 <- keras_model_sequential()
model_0 %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), 
                input_shape = input_shape,
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), 
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.4) %>% 
  
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_flatten() %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = num_classes) %>%
  layer_activation(activation = "softmax")
  
model_0 %>% compile(loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(),
  metrics = c("accuracy"))
history_0 <- model_0 %>%
  fit(x_train, y_train, epochs = 10,
      validation_data = list(x_test, y_test))

predict_train_0 = predict_classes(model_0, x_train)
train_prediction_acc_0 = mean(predict_train_0 == mnist$train$y)
print (train_prediction_acc_0)

predict_valid_0 = predict_classes(model_0, x_test)
valid_prediction_acc_0 = mean(predict_valid_0 == mnist$test$y)
print (valid_prediction_acc_0 )
```
according to the ouput, the in sample prediction is 0.9828333, and the out out sample predication accuracy is 0.9815.
Therefore, we may see that CNN method greatly improve the accuracy of predication in the case.

#Question 2
To change to parameters, we firstly need to have the original model from 8.10.4ï¼Œreference to the code in textbook
```{r}
# import the data
y_train <- data.frame(train_id="train", class=y_train, class_name=letters[as.numeric(y_train)])
y_valid <- data.frame(train_id="test", class=y_test, class_name=letters[as.numeric(y_test)])
emnist <- rbind(y_train, y_valid)
# Reshape x28
x28 <- rbind(x_train, x_test)
dim(x28) <- c(120000, 28, 28, 1)
#change to categorical value of the class
Y <- to_categorical(emnist$class)
#split the data
X <- t(apply(x28, 1, cbind))
X_train <- X[emnist$train_id == "train",]
dim(X_train) <- c(60000, 28, 28, 1)
X_valid <- X[emnist$train_id != "train",]
dim(X_valid) <- c(10000, 28, 28, 1)
Y_train <- Y[emnist$train_id == "train",]
Y_valid <- Y[emnist$train_id != "train",]

# original model
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
input_shape = c(28, 28, 1),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.5) %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.5) %>%
layer_flatten() %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 26) %>%
layer_activation(activation = "softmax")
# Compiling and fitting a convolutional neural network
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))
history <- model %>% 
  fit(X_train, Y_train, epochs = 5, validation_data = list(X_valid, Y_valid))
# Prediction accuracy
emnist$predict <- predict_classes(model, X)
tapply(emnist$predict == emnist$class, emnist$train_id,mean) 

```
According to the output, we have the in-sample accuracy = 0.90436, validation accuracy = 0.87455. 
To imrove the accrucary, we will here change:
1) the drop out rate to 0.3
2) the kernal size from c(2,2) to c(4,4)
```{r}
#the new model
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32, kernel_size = c(4,4),
input_shape = c(28, 28, 1),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(4,4),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.5) %>%
layer_conv_2d(filters = 32, kernel_size = c(4,4),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(4,4),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(4, 4)) %>%
layer_dropout(rate = 0.3) %>%
layer_flatten() %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 26) %>%
layer_activation(activation = "softmax")
# Compiling and fitting a convolutional neural network
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))
history <- model %>% 
  fit(X_train, Y_train, epochs = 5, validation_data = list(X_valid, Y_valid))
# Prediction accuracy
emnist$predict <- predict_classes(model, X)
tapply(emnist$predict == emnist$class, emnist$train_id,mean) 
```
According to the output, by adjusting the kernel size and drop-out rate, we improved the accuracy,
we have the in-sample accuracy from 0.90436 to 0.91223, validation accuracy from 0.87455 to 0.89065. 

#Question 3
Reference to textbook Page 211, and the code solution by Honyu Li, Yin Guo, Justina Xie.
P.S.: To be honest, I actually did not know how to solve this question, I fully understood it by learning the ways other classmate solved it on Github. Here I forked the code, and revised it to be more comprehensive and concise. 
For the original code, please check:
https://github.com/JustinaXie/bis557/blob/master/vignettes/homework-5.Rmd
https://github.com/hongyuli94/bis557/blob/master/vignettes/homework-5.rmd
https://github.com/guoyin1688/bis557/blob/master/vignettes/homework-5.Rmd


We first create a function called casl_nn_make_weights, which allows to 
creates such a list, filled with randomly generated values from a normal distribution.

```{r}

# Create list of weights to describe a dense neural network.
#
# Args:
#     sizes: A vector giving the size of each layer, including
#            the input and output layers.
#
# Returns:
#     A list containing initialized weights and biases.
casl_nn_make_weights <- function(sizes) {
  L <- length(sizes) - 1L
  weights <- vector("list", L)
  for (j in seq_len(L)) {
    w <- matrix(rnorm(sizes[j] * sizes[j + 1L]),
                ncol = sizes[j],
                nrow = sizes[j + 1L])
    weights[[j]] <- list(w=w, b=rnorm(sizes[j + 1L]))
  }
  weights
}
```
On the second step, we define ReLU function, which is way of forward passing
```{r}
# Apply a rectified linear unit (ReLU) to a vector/matrix.
#
# Args: 
#     v: A numeric vector or matrix.
#
# Returns:
#     The original input with negative values truncated to zero.
casl_util_ReLU <- function(v) {
  v[v < 0] <- 0
  v
}
```
Based on the previous function, we are now able to deduce the derivative of the ReLU function 
for the backwards pass, as follows
```{r}
# Apply derivative of the rectified linear unit (ReLU).
#
# Args:
#     v: A numeric vector or matrix.
#
# Returns:
#     Sets positive values to 1 and negative values to zero.
casl_util_ReLU_p <- function(v) {
  p <- v * 0
  p[v > 0] <- 1
  p
}
```
then we use the mean absolute deviation to differentiate the loss function for backpropagation
```{r}
# Derivative of the mean absolute deviance (MAD) function.
#
# Args:
#     y: A numeric vector of responses.
#     a: A numeric vector of predicted responses.
#
# Returns:
#     Returned current derivative the MAD function.
casl_util_mse_p <- function(y, a){
  mad <- NULL
  for(i in 1:length(a)){
    if(a[i] >= y[i]) {
      mad[i] = 1
    }
    else {
      mad[i] = -1
    }
  }
  mad
}
```
On the next step, we create a function called nn_forwar_prop, which can accept a generic loss function derivative. 

```{r}
# Apply forward propagation to a set of NN weights and biases.
#
# Args:
#     x: A numeric vector representing one row of the input.
#     weights: A list created by nn_make_weights.
#     sigma: The activation function.
#
# Returns:
#     A list containing the new weighted responses (z) and activations (a).
casl_nn_forward_prop <- function(x, weights, sigma) {
  L <- length(weights)
  z <- vector("list", L)
  a <- vector("list", L)
  for (j in seq_len(L)) {
    a_j1 <- if(j == 1) x else a[[j - 1L]]
    z[[j]] <- weights[[j]]$w %*% a_j1 + weights[[j]]$b
    a[[j]] <- if (j != L) sigma(z[[j]]) else z[[j]]
  }
  list(z=z, a=a)
}
```
In the following step, we create the function called nn_backward_prop, which will accepts the output of the forward pass
and functions by giving the derivatives of the loss and activation functions,

```{r}
# Apply backward propagation algorithm.
#
# Args:
#     x: A numeric vector representing one row of the input.
#     y: A numeric vector representing one row of the response.
#     weights: A list created by nn_make_weights.
#     f_obj: Output of the function nn_forward_prop.
#     sigma_p: Derivative of the activation function.
#     f_p: Derivative of the loss function.
#
# Returns:
#     A list containing the new weighted responses (z) and activations (a).
casl_nn_backward_prop <- function(x, y, weights, f_obj, sigma_p, f_p) {
  z <- f_obj$z
  a <- f_obj$a
  L <- length(weights)
  grad_z <- vector("list", L)
  grad_w <- vector("list", L)
  for (j in rev(seq_len(L))) {
    if (j == L) {
      grad_z[[j]] <- f_p(y, a[[j]])
    } 
    else {
      grad_z[[j]] <- (t(weights[[j + 1]]$w) %*% grad_z[[j + 1]]) * sigma_p(z[[j]])
    }
    a_j1 <- if(j == 1) x else a[[j - 1L]]
    grad_w[[j]] <- grad_z[[j]] %*% t(a_j1)
  }
  list(grad_z=grad_z, grad_w=grad_w)
}
```
According to the output, the backward propagation function will produce a list of gradients with respect to $z$ and $w$ 
In the next step, another function is written that will produce a list containing the trained weights for the network.

```{r}
# Apply stochastic gradient descent (SGD) to estimate NN.
#
# Args:
#     X: A numeric data matrix.
#     y: A numeric vector of responses.
#     sizes: A numeric vector giving the sizes of layers in the neural network.
#     epochs: Integer number of epochs to computer.
#     eta: Positive numeric learning rate.
#     f_p: Derivative of the loss function.
#     weights: Optional list of starting weights.
#
# Returns:
#     A list containing the trained weights for the network.
casl_nn_sgd <- function(X, y, sizes, epochs, eta, weights=NULL) {
  if (is.null(weights)) {
    weights <- casl_nn_make_weights(sizes)
  }
  for (epoch in seq_len(epochs)) {
    for (i in seq_len(nrow(X))) {
      f_obj <- casl_nn_forward_prop(X[i,], weights, casl_util_ReLU)
      b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
                                     f_obj, casl_util_ReLU_p,
                                     casl_util_mse_p)
      for (j in seq_along(b_obj)) {
        weights[[j]]$b <- weights[[j]]$b - eta * b_obj$grad_z[[j]]
        weights[[j]]$w <- weights[[j]]$w - eta * b_obj$grad_w[[j]]
      }
    }
  }
  weights
}
```
Last but not least, we need another function that can output a matrix of predicted values
```{r}
# Predict values from a training neural network.
#
# Args:
#     weights: List of weights describing the neural network.
#     X_test: A numeric data matrix for the predictions.
#
# Returns:
#     A matrix of predicted values.
casl_nn_predict <- function(weights, X_test) {
  p <- length(weights[[length(weights)]]$b)
  y_hat <- matrix(0, ncol = p, nrow = nrow(X_test))
  for (i in seq_len(nrow(X_test))) {
    a <- casl_nn_forward_prop(X_test[i,], weights, casl_util_ReLU)$a
    y_hat[i,] <- a[[length(a)]]
  }
  y_hat
}
```

After creating all of the functions, we can now test the use of this function with a simulation containing several outliers. And then check how do neural networks and SGD perform
```{r}
set.seed(1234)
# create x and y
X = matrix(runif(1000, min=-1, max=1), ncol=1)
ys = X[,1,drop = FALSE]^2 + rnorm(1000, sd = 0.1)

# set some outliers
id = sample(seq_along(ys), 100)
ys[id] = c(runif(100, -15, -10))

# apply the function
weights = casl_nn_sgd(X, ys, sizes = c(1, 25, 1), epochs=10, eta=0.001)
y_pred = casl_nn_predict(weights, X)

## plot the predict values
df = tibble(x = as.vector(X), y_pred = as.vector(y_pred),
             y = X[,1]^2, ys = as.vector(ys))
ggplot(df) + 
  geom_point(aes(x = x, y = ys)) +
  geom_line(aes(x = x, y = y_pred), color="blue") +
  labs(x = "x", y = "y") + 
  ggtitle("Output that using Mean Absolute Deviation (MAD)")

```

To compare with the method of using mean square error, we may change the function that:

```{r}
# Derivative of the mean squared error (MSE) function.
#
# Args:
#     y: A numeric vector of responses.
#     a: A numeric vector of predicted responses.
#
# Returns:
#     Returned current derivative the MSE function.
# 
casl_util_mse_p <- function(y, a) {
  2 * (a - y)
}
```
And now we perfom the test again, to compare the graphs:
```{r}
weights = casl_nn_sgd(X, ys, sizes = c(1, 25, 1), epochs=10, eta=0.001)
y_pred = casl_nn_predict(weights, X)

## plot the predict values
df = tibble(x = as.vector(X), y_pred = as.vector(y_pred),
             y = X[,1]^2, ys = as.vector(ys))
ggplot(df) + 
  geom_point(aes(x = x, y = ys)) +
  geom_line(aes(x = x, y = y_pred), color="blue") +
  labs(x = "x", y = "y") + 
  ggtitle("Output that using Mean Square Error (MSE)")
```

From the output, we may notice that the method adopting mean absolute deviation will greatly improve the model,
as using it as a loss function.