---
title: <center> <h1> Homework 5 </h1> </center>
mainfont: Arial
author: "Boxiang Jia"
date: December 4, 2018
output: html_document
runtime: shiny 
fontsize: 12pt
geometry: margin=1in
self_contained: yes
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{homework 5 vignette}
-->

#Question 1
```{r}
# install and load packages
library(keras)
# install_keras() if doesn't have the keras and tensorflow package
library(keras)
library(glmnet)
library(moments)
library(dplyr)
library(ggplot2)
library(tibble)

# split the data into discovey and validation set, and reshape 
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

x_train <- array_reshape(x_train, c(60000, 28^2))
x_test <- array_reshape(x_test, c(10000, 28^2))
y_train <- factor(y_train)
y_test <- factor(y_test)

# build the model
set.seed(1234)

s <- sample(seq_along(y_train), 1000)
fit <- cv.glmnet(x_train[s,], y_train[s], family = "multinomial")

# fit the model into the validation set
preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda.min,  type = "class")
t <- table(as.vector(preds), y_test)
sum(diag(t)) / sum(t)
```
The prediction accuracy of the way above is 0.8556, and to increase the out of sample predication accuracy,
we may add other features, credit to the thoughts from Guoyin and online search, we will add mean and standard deviation
as the new featurea to predict the model.

```{r}
x_train2 <- cbind(x_train, as.vector(apply(x_train, 1, mean)), as.vector(apply(x_train, 1, sd)))
x_test2 <- cbind(x_test, as.vector(apply(x_test, 1, mean)), as.vector(apply(x_test, 1, sd)))

# build the  model and predict
fit2 <- cv.glmnet(x_train2[s,], y_train[s], family = "multinomial")
preds2 <- predict(fit2$glmnet.fit, x_test2, s = fit2$lambda.min, type = "class")
t2 <- table(as.vector(preds2), y_test)
sum(diag(t2)) / sum(t2)
```
The accuracy has been improved from 0.8556 to 0.8566, which is a slight improvement.
Moreover, we could also use the Convolutional Neural Network(CNN) to predict. I actually do not have an idea how to perform it.
Credit to the code of BIS557 classmate Hongyu Li: https://github.com/hongyuli94/bis557/blob/master/vignettes/homework-5.rmd
I forked the code here, and add some revision to make it  more conise and run more quickly.

```{r}
# set the dimensions
img_rows <- 28
img_cols <- 28
num_classes <- 10
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)

# refine RGB value, setting between 0 and 1
x_train <- x_train / 255
x_test <- x_test / 255

# Convert class vectors to binary 
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)

# build the model of sequantial analysis
model_0 <- keras_model_sequential()
model_0 %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), 
                input_shape = input_shape,
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), 
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.4) %>% 
  
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_flatten() %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = num_classes) %>%
  layer_activation(activation = "softmax")
  
model_0 %>% compile(loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(),
  metrics = c("accuracy"))
history_0 <- model_0 %>%
  fit(x_train, y_train, epochs = 10,
      validation_data = list(x_test, y_test))

predict_train_0 = predict_classes(model_0, x_train)
train_prediction_acc_0 = mean(predict_train_0 == mnist$train$y)
print (train_prediction_acc_0)

predict_valid_0 = predict_classes(model_0, x_test)
valid_prediction_acc_0 = mean(predict_valid_0 == mnist$test$y)
print (valid_prediction_acc_0 )
```
according to the ouput, the in sample prediction is 0.9828333, and the out out sample predication accuracy is 0.9815.
Therefore, we may see that CNN method greatly improve the accuracy of predication in the case.

#Question 2
To change to parameters, we firstly need to have the original model from 8.10.4ï¼Œreference to the code in textbook
```{r}
# import the data
y_train <- data.frame(train_id="train", class=y_train, class_name=letters[as.numeric(y_train)])
y_valid <- data.frame(train_id="test", class=y_test, class_name=letters[as.numeric(y_test)])
emnist <- rbind(y_train, y_valid)
# Reshape x28
x28 <- rbind(x_train, x_test)
dim(x28) <- c(120000, 28, 28, 1)
#change to categorical value of the class
Y <- to_categorical(emnist$class)
#split the data
X <- t(apply(x28, 1, cbind))
X_train <- X[emnist$train_id == "train",]
dim(X_train) <- c(60000, 28, 28, 1)
X_valid <- X[emnist$train_id != "train",]
dim(X_valid) <- c(10000, 28, 28, 1)
Y_train <- Y[emnist$train_id == "train",]
Y_valid <- Y[emnist$train_id != "train",]

# original model
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
input_shape = c(28, 28, 1),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.5) %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.5) %>%
layer_flatten() %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 26) %>%
layer_activation(activation = "softmax")
# Compiling and fitting a convolutional neural network
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))
history <- model %>% 
  fit(X_train, Y_train, epochs = 5, validation_data = list(X_valid, Y_valid))
# Prediction accuracy
emnist$predict <- predict_classes(model, X)
tapply(emnist$predict == emnist$class, emnist$train_id,mean) 

```
According to the output, we have the in-sample accuracy = 0.90436, validation accuracy = 0.87455. 
To imrove the accrucary, we will here change:
1) the drop out rate to 0.3
2) the kernal size from c(2,2) to c(4,4)
```{r}
#the new model
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32, kernel_size = c(4,4),
input_shape = c(28, 28, 1),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(4,4),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.5) %>%
layer_conv_2d(filters = 32, kernel_size = c(4,4),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(4,4),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(4, 4)) %>%
layer_dropout(rate = 0.3) %>%
layer_flatten() %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 26) %>%
layer_activation(activation = "softmax")
# Compiling and fitting a convolutional neural network
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))
history <- model %>% 
  fit(X_train, Y_train, epochs = 5, validation_data = list(X_valid, Y_valid))
# Prediction accuracy
emnist$predict <- predict_classes(model, X)
tapply(emnist$predict == emnist$class, emnist$train_id,mean) 
```
According to the output, by adjusting the kernel size and drop-out rate, we improved the accuracy,
we have the in-sample accuracy from 0.90436 to 0.91223, validation accuracy from 0.87455 to 0.89065. 

#Question 3
```{r}
# Create list of weights to describe a dense neural network.
#
# Args:
#     sizes: A vector giving the size of each layer, including
#            the input and output layers.
#
# Returns:
#     A list containing initialized weights and biases.
nn_make_weights <- function(sizes) {
  L <- length(sizes) - 1L
  weights <- vector("list", L)
  for (j in seq_len(L)) {
    w <- matrix(rnorm(sizes[j] * sizes[j + 1L]), 
                ncol = sizes[j], nrow = sizes[j + 1L])
    weights[[j]] <- list(w = w, b = rnorm(sizes[j + 1L]))
  }
  weights
}
```

```{r}
# Apply a rectified linear unit (ReLU) to a vector/matrix.
#
# Args: 
#     v: A numeric vector or matrix.
#
# Returns:
#     The original input with negative values truncated to zero.
ReLU <- function(v) {
  v[v < 0] <- 0
  v
}
```

```{r}
# Apply derivative of the rectified linear unit (ReLU).
#
# Args:
#     v: A numeric vector or matrix.
#
# Returns:
#     Sets positive values to 1 and negative values to zero.
ReLU_p <- function(v) {
  p <- v * 0
  p[v > 0] <- 1
  p
}
```

```{r}
# Derivative of the mean absolute deviance (MAD) function.
#
# Args:
#     y: A numeric vector of responses.
#     a: A numeric vector of predicted responses.
#
# Returns:
#     Returned current derivative the MAD function.
mad_p <- function(y, a) {
  p <- rep(NA, length(a))
  for (i in 1:length(a)){
    if (y[i] <= a[i]) 
      p[i] <- 1
    else 
      p[i] <- -1
  }
  p
}
```

```{r}
# Derivative of the mean squared error (MSE) function.
# Args:
#     y: A numeric vector of responses.
#     a: A numeric vector of predicted responses.
#
# Returns:
#     Returned current derivative the MSE function.
mse_p <- function(y, a) {
  1/2*(a - y)
}
```

```{r}
# Apply forward propagation to a set of NN weights and biases.
#
# Args:
#     x: A numeric vector representing one row of the input.
#     weights: A list created by nn_make_weights.
#     sigma: The activation function.
#
# Returns:
#     A list containing the new weighted responses (z) and activations (a).
nn_forward_prop <- function(x, weights, sigma) {
  L <- length(weights)
  z <- vector("list", L)
  a <- vector("list", L)
  for (j in seq_len(L)) {
  a_j1 <- if(j == 1) x else a[[j - 1L]]
  z[[j]] <- weights[[j]]$w %*% a_j1 + weights[[j]]$b
  a[[j]] <- if(j != L) sigma(z[[j]]) else z[[j]]
  }
  list(z = z, a = a)
}
```

```{r}
# Apply backward propagation algorithm.
#
# Args:
#     x: A numeric vector representing one row of the input.
#     y: A numeric vector representing one row of the response.
#     weights: A list created by nn_make_weights.
#     f_obj: Output of the function nn_forward_prop.
#     sigma_p: Derivative of the activation function.
#     f_p: Derivative of the loss function.
#
# Returns:
#     A list containing the new weighted responses (z) and activations (a).
nn_backward_prop <- function(x, y, weights, f_obj, sigma_p, f_p) {
  z <- f_obj$z
  a <- f_obj$a
  L <- length(weights)
  grad_z <- vector("list", L)
  grad_w <- vector("list", L)
  for (j in rev(seq_len(L))) {
    if (j == L) {
      grad_z[[j]] <- f_p(y, a[[j]])
      } else {
        grad_z[[j]] <- (t(weights[[j + 1]]$w) %*% grad_z[[j + 1]]) * 
          sigma_p(z[[j]])
        }
    a_j1 <- if(j == 1) x else a[[j - 1L]]
    grad_w[[j]] <- grad_z[[j]] %*% t(a_j1)
    }
  list(grad_z = grad_z, grad_w = grad_w)
}
```

```{r}
# Apply stochastic gradient descent (SGD) to estimate NN.
#
# Args:
#     X: A numeric data matrix.
#     y: A numeric vector of responses.
#     sizes: A numeric vector giving the sizes of layers in the neural network.
#     epochs: Integer number of epochs to computer.
#     eta: Positive numeric learning rate.
#     f_p: Derivative of the loss function.
#     weights: Optional list of starting weights.
#
# Returns:
#     A list containing the trained weights for the network.
nn_sgd <- function(X, y, sizes, epochs, eta, f_p, weights=NULL) {
  if (is.null(weights)) {
    weights <- nn_make_weights(sizes)
  }
  
  for (epoch in seq_len(epochs)){
    for (i in seq_len(nrow(X))){ 
      f_obj <- nn_forward_prop(X[i,], weights, ReLU)
      b_obj <- nn_backward_prop(X[i,], y[i,], weights,
                                     f_obj, ReLU_p, f_p)
      for (j in seq_along(weights)){
        weights[[j]]$b <- weights[[j]]$b -eta * b_obj$grad_z[[j]]
        weights[[j]]$w <- weights[[j]]$w -eta * b_obj$grad_w[[j]]
      }
    }
  }
  weights
}
```

```{r}
# Predict values from a training neural network.
#
# Args:
#     weights: List of weights describing the neural network.
#     X_test: A numeric data matrix for the predictions.
#
# Returns:
#     A matrix of predicted values.
nn_predict <- function(weights, X_test) {
  p <- length(weights[[length(weights)]]$b)
  y_hat <- matrix(0, ncol = p, nrow = nrow(X_test))
  for (i in seq_len(nrow(X_test))) {
    a <- nn_forward_prop(X_test[i,], weights, ReLU)$a
    y_hat[i, ] <- a[[length(a)]]
  }
  y_hat
}
```

We test the use of this function with a simulation containing several outliers.

```{r}
set.seed(1222)
X <- matrix(runif(1000, min = -1, max = 1), ncol = 1)
y <- X[, 1, drop = FALSE]^2 + rnorm(1000, sd = 0.1)
# Adding several outliers
ind <- sample(seq_along(y), 100)
y[ind] <- c(runif(50, -10, -5), runif(50, 5, 10))
weights <- nn_sgd(X, y, sizes = c(1, 25, 1), epochs = 15, eta = 0.01, mad_p)
y_pred <- nn_predict(weights, X)
weights2 <- nn_sgd(X, y, sizes = c(1, 25, 1), epochs = 15, eta = 0.01, mse_p)
y_pred2 <- nn_predict(weights2, X)
df <- tibble(X = as.vector(X), y_pred = as.vector(y_pred), 
             y_pred2 = as.vector(y_pred2), y = as.vector(y))
ggplot(df) + geom_point(aes(x = X, y = y)) + ylim(c(-1, 2)) + 
  geom_point(aes(x = X, y = y_pred), color = "red") + 
  geom_point(aes(x = X, y = y_pred2), color = "blue") + 
  labs(x = "X", y = "True/Predicted Values", 
       subtitle="Black: True; Red: MAD Predicted; Blue: MSE Predicted")
```

For better visualization, the above plot does not show the large outliers. We can see that in terms of robustness, MAD (red line) performs better than MSE (blue line). 

